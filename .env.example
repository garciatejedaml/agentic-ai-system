# ── Agentic AI System — Environment Variables ─────────────────────────────────
# Copy to .env and fill in your values:
#   cp .env.example .env

# ── LLM Provider ──────────────────────────────────────────────────────────────
# "anthropic" = Anthropic API (best quality, requires API key below)
# "ollama"    = Local Ollama (free, no API key needed — see Ollama section below)
# "bedrock"   = AWS Bedrock (production, requires IAM role)
LLM_PROVIDER=anthropic

# ── Anthropic API (LLM_PROVIDER=anthropic) ────────────────────────────────────
# Get a key at https://console.anthropic.com
# Not needed if LLM_PROVIDER=ollama
ANTHROPIC_API_KEY=sk-ant-...

# Model overrides (defaults to latest Sonnet + Haiku)
ANTHROPIC_MODEL=claude-sonnet-4-6
ANTHROPIC_FAST_MODEL=claude-haiku-4-5-20251001

# ── Ollama (LLM_PROVIDER=ollama) — free, no API key required ──────────────────
# Install on Mac: brew install ollama && ollama serve
# Pull a model:   ollama pull llama3.2:3b
#
# Docker containers reach native Ollama via host.docker.internal (set automatically).
# Switch to http://ollama:11434 only when using the Docker Ollama profile.
#
# OLLAMA_BASE_URL=http://host.docker.internal:11434
# OLLAMA_MODEL=llama3.2:3b
# OLLAMA_FAST_MODEL=

# ── Optional: observability (Langfuse) ────────────────────────────────────────
# Leave empty to disable tracing (OBSERVABILITY_ENABLED defaults to false)
LANGFUSE_PUBLIC_KEY=
LANGFUSE_SECRET_KEY=

# ── Optional: web search tool ─────────────────────────────────────────────────
# Get a free key at https://brave.com/search/api/
BRAVE_API_KEY=

# ── Optional: AMPS product topic host overrides ───────────────────────────────
# Only needed if you run AMPS instances outside Docker (e.g. native AMPS install).
# When using docker-compose.local.yml these are set automatically.
# AMPS_PORTFOLIO_HOST=localhost
# AMPS_PORTFOLIO_PORT=9008
# AMPS_CDS_HOST=localhost
# AMPS_CDS_PORT=9009
# AMPS_ETF_HOST=localhost
# AMPS_ETF_PORT=9010
# AMPS_RISK_HOST=localhost
# AMPS_RISK_PORT=9011
